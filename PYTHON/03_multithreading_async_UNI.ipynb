{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d244dd8",
   "metadata": {},
   "source": [
    "\n",
    "# 03 — Concurrence & Parallélisme en Python (Version **Université**)\n",
    "\n",
    "> **Objectif pédagogique** : à la fin de ce chapitre, tu dois être capable d’**expliquer** (oral d’entretien) et **implémenter** (live coding) un pipeline concurrent **correct** en Python, choisir entre **threads**, **processus**, et **asyncio**, et justifier tes choix (I/O‑bound vs CPU‑bound, GIL, coûts de pickling, backpressure, timeouts).\n",
    "\n",
    "### Plan du cours\n",
    "1. **Motivation & vocabulaire** (concurrence vs parallélisme, latence vs débit)  \n",
    "2. **Le GIL** (définition, conséquences, mythes)  \n",
    "3. **Threading** (quand, pourquoi, design patterns : producer/consumer, backpressure, poison pill)  \n",
    "4. **Multiprocessing** (vrai parallélisme CPU, pickling, chunking, shared memory)  \n",
    "5. **Asyncio** (concurrence coopérative, event loop, cancellation, timeouts)  \n",
    "6. **Étude de cas “feed → stratégie → OMS”** (combiner les modèles proprement)  \n",
    "7. **Pièges, bonnes pratiques, checklist d’entretien**  \n",
    "8. **Exercices guidés** + corrigés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9710440f",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Motivation & vocabulaire (5 min)\n",
    "\n",
    "**Pourquoi la concurrence ?**  \n",
    "Dans un moteur de trading, tu dois souvent **consommer** des données (ticks, carnets d’ordres), **calculer** des signaux, et **émettre** des ordres **en parallèle** pour minimiser la **latence** et maximiser le **débit**.\n",
    "\n",
    "**Concurrence vs Parallélisme**  \n",
    "- **Concurrence** : organiser des tâches qui se **chevauchent** dans le temps (pas nécessairement exécutées en même temps).  \n",
    "- **Parallélisme** : exécuter **vraiment en même temps** sur plusieurs cœurs.  \n",
    "Python (CPython) offre les deux, via des **threads** (concurrence utile en I/O), des **processus** (parallélisme CPU) et **asyncio** (concurrence coopérative mono‑thread).\n",
    "\n",
    "**Latence vs Débit**  \n",
    "- **Latence** = temps de réponse pour une tâche unique.  \n",
    "- **Débit (throughput)** = nombre de tâches traitées par unité de temps.  \n",
    "Un bon design doit préciser ce qu’on optimise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263426c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Le GIL (Global Interpreter Lock) — ce qu’il faut dire à l’oral\n",
    "\n",
    "- Le **GIL** est un verrou global de l’interpréteur **CPython** : à un instant donné, **un seul thread** exécute du **bytecode Python**.  \n",
    "- **Implication** : pour du **CPU pur** (boucles Python lourdes), les threads **n’accélèrent pas** (ils se partagent le GIL).  \n",
    "- **Mais** : les opérations **I/O** (réseau, disque) **libèrent** le GIL → les threads peuvent **vraiment** se chevaucher en I/O (bon pour HTTP, sockets, DB).  \n",
    "- Beaucoup de libs natives (NumPy) exécutent du C qui **libère** le GIL pendant l’opération : un thread peut alors découpler Python↔C.\n",
    "\n",
    "**Résumé décidable**  \n",
    "- **I/O‑bound** → `threading` **ou** `asyncio`  \n",
    "- **CPU‑bound** → `multiprocessing` (ou Numba/Cython)\n",
    "\n",
    "**Mythes fréquents**  \n",
    "- “Le GIL empêche toute concurrence” ❌ Faux : il empêche seulement l’exécution **CPU Python** simultanée, pas l’**overlap d’I/O**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f0b88",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Threading — Producer/Consumer, backpressure, poison pill \n",
    "\n",
    "### Quand utiliser les **threads** ?\n",
    "- Tâches **I/O‑bound** : lectures socket, appels réseau, écriture disque, DB.  \n",
    "- Tu veux un modèle pragmatique, simple à lire, sans dépendre de l’event loop `asyncio`.\n",
    "\n",
    "### Pourquoi une **queue** ?\n",
    "- **Découpler** producteur(s) et consommateur(s).  \n",
    "- Implémenter une **régulation (backpressure)** via `maxsize` : si les consommateurs ralentissent, le producteur **bloque** sur `put()`, évitant de saturer la mémoire.  \n",
    "- Faciliter l’**arrêt propre** via **poison pill** (sentinelle `None`).\n",
    "\n",
    "### Schéma mental\n",
    "```\n",
    "[Producer(s)] --put()--> [ queue.Queue(maxsize=M) ] --get()--> [Consumer(s)]\n",
    "   ↑ backpressure si file pleine                  ↑ poison pill pour arrêter\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e923d1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,\n",
       " [(0, 0, 0.7405738344840216),\n",
       "  (1, 3, 0.2692098905282778),\n",
       "  (3, 2, 0.32296949252872076),\n",
       "  (2, 1, 1.1266421518297678),\n",
       "  (2, 7, 1.8436315658140712)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Threading : pipeline robuste avec backpressure & poison pill\n",
    "import threading, time, queue, random\n",
    "\n",
    "q = queue.Queue(maxsize=200)    # régulation\n",
    "results = []\n",
    "N_ITEMS = 2000\n",
    "\n",
    "def producer(n=N_ITEMS):\n",
    "    for i in range(n):\n",
    "        q.put((i, random.random()))  # bloque si maxsize atteint\n",
    "    for _ in range(4):               # 4 consommateurs => 4 pilules\n",
    "        q.put(None)                  # poison pill\n",
    "\n",
    "def consumer(cid):\n",
    "    while True:\n",
    "        item = q.get()               # bloque si vide\n",
    "        if item is None:\n",
    "            q.task_done()\n",
    "            break\n",
    "        i, x = item\n",
    "        # Travail I/O simulé\n",
    "        time.sleep(0.0005)\n",
    "        results.append((cid, i, x*2))\n",
    "        q.task_done()\n",
    "\n",
    "threads = [threading.Thread(target=consumer, args=(k,)) for k in range(4)]\n",
    "for t in threads: t.start()\n",
    "producer()\n",
    "q.join()                             # attend le traitement complet\n",
    "for t in threads: t.join()\n",
    "\n",
    "len(results), results[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8410430",
   "metadata": {},
   "source": [
    "\n",
    "### Sections critiques & **race conditions**\n",
    "Dès qu’on **partage** un état (ex : un compteur), deux threads peuvent écrire **en même temps** → **race condition**.\n",
    "\n",
    "**Remède** : un **verrou** (`threading.Lock`) entourant la **plus petite** portion de code qui manipule l’état (section critique).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5584902c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Démonstration lock minimal\n",
    "import threading\n",
    "\n",
    "counter = 0\n",
    "lock = threading.Lock()\n",
    "\n",
    "def incr(n=10000):\n",
    "    global counter\n",
    "    for _ in range(n):\n",
    "        # Protéger seulement ce qui est nécessaire\n",
    "        with lock:\n",
    "            counter += 1\n",
    "\n",
    "threads = [threading.Thread(target=incr) for _ in range(8)]\n",
    "[t.start() for t in threads]\n",
    "[t.join() for t in threads]\n",
    "counter  # doit être 80000 si tout va bien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7635a2",
   "metadata": {},
   "source": [
    "\n",
    "**Bonnes pratiques Threading (à citer)**\n",
    "- Préférer la **communication par messages** (queues) aux verrous sophistiqués.  \n",
    "- Les traitements doivent être **idempotents** (rejouables sans casser l’état).  \n",
    "- Gérer l’**arrêt** (poison pill), les **timeouts** et la **journalisation** (logs).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c9a5a",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Multiprocessing — vrai parallélisme CPU (12 min)\n",
    "\n",
    "### Quand utiliser **multiprocessing** ?\n",
    "- Tâches **CPU‑bound** (calcul numérique pur en Python) qui ne bénéficient pas d’une lib C optimisée.  \n",
    "- Exploiter **plusieurs cœurs** sans GIL partagé.\n",
    "\n",
    "### Coûts/Contraintes\n",
    "- **Pickling** : les données échangées entre process sont **sérialisées** → coût non négligeable.  \n",
    "- **Spawn/Fork** : lancement de process coûteux (surtout `spawn` sur Windows/macOS).  \n",
    "- Favoriser des **gros “chunks”** de travail (coarse‑grained) pour amortir les coûts.\n",
    "\n",
    "### Exemple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from multiprocessing import Pool\n",
    "import math, time\n",
    "\n",
    "def cpu_heavy(n):\n",
    "    s=0.0\n",
    "    for i in range(10_000):\n",
    "        s += math.sqrt((i*n) % 97)\n",
    "    return s\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "with Pool(4) as p:\n",
    "    res = p.map(cpu_heavy, [10,11,12,13])\n",
    "elapsed = time.perf_counter()-t0\n",
    "len(res), f\"{elapsed:.3f}s\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ccd49",
   "metadata": {},
   "source": [
    "\n",
    "**Tips “entretien”**\n",
    "- Les objets envoyés à un worker doivent être **picklables**.  \n",
    "- **Chunking** : grouper des tâches plutôt que d’envoyer 1 micro‑tâche = 1 message.  \n",
    "- En data science, voir aussi **shared_memory** (tableaux partagés) pour limiter les copies.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f98a2",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Asyncio — Concurrence coopérative mono‑thread (15 min)\n",
    "\n",
    "### Pourquoi **asyncio** ?\n",
    "- Des milliers de **petites I/O** concurrentes (HTTP, websockets) avec un **faible overhead**.  \n",
    "- Contrôle fin : **timeouts**, **cancellation**, **gather**/`TaskGroup` (3.11+).\n",
    "\n",
    "### Schéma mental\n",
    "```\n",
    "          +-------------------+\n",
    "          |   Event Loop      |\n",
    "await --> |   planifie/ordonne| --> d'autres coroutines progressent\n",
    "          +-------------------+\n",
    "```\n",
    "\n",
    "**Idée clé** : à chaque `await`, la coroutine **rend la main**. Si elle attend I/O, **une autre** peut avancer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38450731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Timeout global avec asyncio.wait_for + gather\n",
    "import asyncio, random\n",
    "\n",
    "async def fetch(symbol):\n",
    "    await asyncio.sleep(random.random()/5)  # I/O simulé\n",
    "    return symbol, 100 + random.random()\n",
    "\n",
    "async def main():\n",
    "    syms = [f\"SYM{i}\" for i in range(10)]\n",
    "    try:\n",
    "        res = await asyncio.wait_for(\n",
    "            asyncio.gather(*[fetch(s) for s in syms]),\n",
    "            timeout=2.0\n",
    "        )\n",
    "        print(\"OK\", len(res))\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"Timeout global\")\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948c87e",
   "metadata": {},
   "source": [
    "\n",
    "### Queue `asyncio` : producer/consumers\n",
    "- Très proche du modèle `queue.Queue`, mais **non bloquant**.  \n",
    "- Parfait pour un **collecteur de ticks** qui pousse dans un bus interne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e504b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio, random\n",
    "\n",
    "async def producer(ch, n=50):\n",
    "    for i in range(n):\n",
    "        await ch.put((i, random.random()))\n",
    "    for _ in range(3):\n",
    "        await ch.put(None)\n",
    "\n",
    "async def consumer(ch, cid):\n",
    "    while True:\n",
    "        item = await ch.get()\n",
    "        if item is None:\n",
    "            ch.task_done()\n",
    "            break\n",
    "        await asyncio.sleep(0.002)  # I/O simulé\n",
    "        ch.task_done()\n",
    "\n",
    "async def run():\n",
    "    ch = asyncio.Queue(maxsize=20)\n",
    "    prod = asyncio.create_task(producer(ch, 100))\n",
    "    cons = [asyncio.create_task(consumer(ch, k)) for k in range(3)]\n",
    "    await asyncio.gather(prod)\n",
    "    await ch.join()\n",
    "    for c in cons:\n",
    "        await c\n",
    "\n",
    "asyncio.run(run())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a26450",
   "metadata": {},
   "source": [
    "\n",
    "**À citer en entretien**\n",
    "- `await` = point de suspension coopératif ; pas de préemption.  \n",
    "- **Timeouts** via `asyncio.wait_for`, **cancellation** via `task.cancel()`.  \n",
    "- **Pas** de data race Python (mono‑thread), mais attention au **partage mutable** entre coroutines.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b662a331",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Étude de cas : **Feed → Stratégie → OMS** (10 min)\n",
    "\n",
    "**Contexte** : tu reçois des ticks (I/O), tu calcules un signal (léger CPU), tu routes un ordre (I/O).  \n",
    "**Design recommandé** (simple & robuste) :\n",
    "```\n",
    "[Ticker Thread/Async] --(queue)--> [Strategy Worker(s)] --(queue)--> [OMS Adapter (I/O)]\n",
    "```\n",
    "- **Entrée** (I/O) : `threading` **ou** `asyncio`  \n",
    "- **Traitement** (léger CPU) : **threads** conviennent (ou même synchrone si simple)  \n",
    "- **Sortie** (I/O vers broker) : **Adapter** + `threading`/`asyncio`\n",
    "\n",
    "**Pourquoi pas `multiprocessing` ici ?**  \n",
    "- Le calcul est léger. Coûts de **pickling** > gains.  \n",
    "- Réserve `multiprocessing` aux tâches **CPU lourdes** (pricing massif offline, backtests).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c00ad0",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Pièges, bonnes pratiques, checklist d’entretien (8 min)\n",
    "\n",
    "**Pièges classiques**\n",
    "- Oublier la **backpressure** (file non bornée → OOM).  \n",
    "- Pas de **poison pill** → threads zombies à l’arrêt.  \n",
    "- Verrous trop larges → **contention** + perf dégradée.  \n",
    "- Pas de **timeout** sur I/O → blocages invisibles.  \n",
    "- Mélanger trop de modèles (threads + async + process) sans raison.\n",
    "\n",
    "**Bonnes pratiques**\n",
    "- **Simplicité d’abord** : un seul modèle si possible.  \n",
    "- **Messages/queues** > partage d’état + verrous.  \n",
    "- **Idempotence** des handlers, **logs** clairs, **metrics** (latence, taille de file, débit).  \n",
    "- Tests : **déterministes** (stubs pour I/O), pas de `sleep` arbitraires (utiliser des queues/événements).\n",
    "\n",
    "**Checklist entretien (à recaser)**\n",
    "- “GIL ⇒ threads pour I/O, `multiprocessing` pour CPU”  \n",
    "- “Pipeline queue avec **maxsize** (backpressure) + **poison pill**”  \n",
    "- “**Timeouts** & **cancellation** en `asyncio`”  \n",
    "- “**Pickling** entre process, penser **chunking** / **shared_memory**”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec5be8",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Exercices guidés (avec corrigés)\n",
    "\n",
    "### Exercice A — Pipeline threads avec mesure de débit\n",
    "**Tâche** : étends l’exemple “producer/consumer” pour afficher le **débit (items/s)** et la **latence moyenne** par item.  \n",
    "**Indice** : timestamp à la production + calcul à la consommation.\n",
    "\n",
    "> 👉 Corrigé ci‑dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ca067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Corrigé Exercice A (simplifié)\n",
    "import time, threading, queue, statistics, random\n",
    "\n",
    "q = queue.Queue(maxsize=500)\n",
    "times = []\n",
    "N = 3000\n",
    "\n",
    "def prod():\n",
    "    for i in range(N):\n",
    "        q.put((i, time.perf_counter()))\n",
    "    for _ in range(4): q.put(None)\n",
    "\n",
    "def cons():\n",
    "    while True:\n",
    "        item = q.get()\n",
    "        if item is None:\n",
    "            q.task_done(); break\n",
    "        i, t0 = item\n",
    "        # Simule un petit travail\n",
    "        time.sleep(0.0003)\n",
    "        times.append(time.perf_counter()-t0)\n",
    "        q.task_done()\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "ts = [threading.Thread(target=cons) for _ in range(4)]\n",
    "for t in ts: t.start()\n",
    "prod()\n",
    "q.join()\n",
    "for t in ts: t.join()\n",
    "elapsed = time.perf_counter()-t0\n",
    "throughput = N/elapsed\n",
    "lat = statistics.mean(times)\n",
    "throughput, lat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09391c37",
   "metadata": {},
   "source": [
    "\n",
    "### Exercice B — `multiprocessing` avec chunking\n",
    "**Tâche** : transforme une liste de 400 “jobs” en 8 chunks de 50 et distribue-les via `Pool.map`.  \n",
    "**But** : illustrer l’amortissement des coûts de messaging/pickling.\n",
    "\n",
    "*(À faire en autonomie — vérifie le temps total avec et sans chunking.)*\n",
    "\n",
    "### Exercice C — `asyncio` + timeout par requête\n",
    "**Tâche** : lance 50 `fetch()` en parallèle avec un **timeout par requête** (pas juste global).  \n",
    "**Indice** : enveloppe **chaque** coroutine avec `wait_for` et gère `TimeoutError` **individuellement**.\n",
    "\n",
    "---\n",
    "\n",
    "## Glossaire express\n",
    "- **Backpressure** : mécanisme qui ralentit la production quand la consommation ne suit pas.  \n",
    "- **Poison pill** : sentinelle pour arrêter proprement un worker.  \n",
    "- **Idempotence** : répéter une action produit le même état final.  \n",
    "- **Pickling** : sérialisation Python pour IPC entre process.  \n",
    "- **Cancellation** : annuler une coroutine/Task en `asyncio`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
